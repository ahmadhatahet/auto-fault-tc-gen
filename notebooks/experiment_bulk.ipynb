{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build testgen Python Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install -e ../."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from testgen.utils import *\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path().cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and rename columns\n",
    "df = pd.read_excel(base_path / \"data/requirements.xlsx\")\n",
    "df_examples = pd.read_excel(base_path / \"data/examples.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all columns except the first one are binary\n",
    "for c in df.columns[1:]:\n",
    "    assert df[c].drop_duplicates().shape[0] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = 1\n",
    "\n",
    "indexes_to_drop, examples = get_examples_from_df(df_examples, N_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all examples in a text format to add to prompt\n",
    "examples_txt = \"\"\n",
    "\n",
    "for e1 in examples.values():\n",
    "    for e2 in e1:\n",
    "        examples_txt += f\"Requirement: {e2[0]}\\n\"\n",
    "        examples_txt += f\"Vector: {e2[1]}\\n\"\n",
    "        examples_txt += f\"Target Sensor/s: {' and '.join(df.columns[1:][[True if x == 1 else False for x in map(int, e2[1][1:-1].split(','))]])}\\n\"\n",
    "        examples_txt += \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement: Power steering systems, whether hydraulic or electronic, must operate effectively in a wide range of environmental conditions, including extreme temperatures\n",
      "Vector: [0,0,0,0,1]\n",
      "Target Sensor/s: steering_torque\n",
      "\n",
      "Requirement: Upon detection of lift-off oversteer, the system shall regain control and prevent spin-out based on the steering angle input and modulate the acceleration pedal \n",
      "Vector: [1,1,0,0,0]\n",
      "Target Sensor/s: acceleration_pedal and wheel_steering_angle\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(examples_txt.split(\"\\n\")[-9:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testgen.prompts import SystemPrompt\n",
    "from testgen.prompts import Sensors\n",
    "from testgen.prompts import UserPromptBulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = {\n",
    "    \"azure\": [\"gpt-4o-mini\"],\n",
    "    \"groq\": [\n",
    "        \"llama-3.2-3b-preview\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "endpoint_attrs = {\n",
    "    \"azure\": {\n",
    "        \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \"api_version\": \"2024-08-01-preview\",\n",
    "        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    },\n",
    "    \"groq\": {\n",
    "        \"api_key\": os.getenv(\"GROQ_API_KEY\"),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"azure\"\n",
    "model_name = llm_models[endpoint_name][0]\n",
    "\n",
    "client = llm_client(endpoint_name, **endpoint_attrs[endpoint_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = client.chat.completions.create(\n",
    "#     model=model_name,\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Respond: Here\"}]\n",
    "# )\n",
    "# t.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Requirements for multi prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches: 68\n",
      "Number of Instances left: 1\n"
     ]
    }
   ],
   "source": [
    "N_REQS = 2\n",
    "SAMPLE_TYPE = \"random\"\n",
    "\n",
    "batches = get_batches(df, SAMPLE_TYPE, N_REQS)\n",
    "batches, req_texts = requirement_text_bulk(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcb0a3036794401bd31d71c0490f424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for batch, req_text in tqdm(zip(batches, req_texts)):\n",
    "    result = invoke_bulk(model_name, client, batch,\n",
    "                         SystemPrompt, Sensors, examples_txt, UserPromptBulk, req_text)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_instances = len(results) * N_REQS\n",
    "\n",
    "accuracy = 0\n",
    "total_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "total_time = 0\n",
    "\n",
    "for r in results:\n",
    "    accuracy += sum(r[\"accuracy\"])\n",
    "    total_tokens += r[\"total_tokens\"]\n",
    "    total_completion_tokens += r[\"completion_tokens\"]\n",
    "    total_time += r['response_time']\n",
    "\n",
    "number_of_reqs = len(results)\n",
    "accuracy /= total_number_of_instances\n",
    "avg_time_per_req = round(total_time / total_number_of_instances, 6)\n",
    "avg_token_per_req = total_tokens / total_number_of_instances\n",
    "avg_completion_token_per_req = total_completion_tokens / total_number_of_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_reqs: 68\n",
      "accuracy: 0.8897058823529411\n",
      "avg_time_per_req: 0.476348\n",
      "avg_token_per_req: 436.375\n",
      "avg_completion_token_per_req: 15.5\n"
     ]
    }
   ],
   "source": [
    "print(\"number_of_reqs:\", number_of_reqs)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(\"avg_time_per_req:\", avg_time_per_req)\n",
    "print(\"avg_token_per_req:\", avg_token_per_req)\n",
    "print(\"avg_completion_token_per_req:\", avg_completion_token_per_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "time = dt.now()\n",
    "\n",
    "results_path = \"results/bulk-{bulk}_{model}_n-{examples}_acc-{accuracy}_{time}.json\"\n",
    "results_path = results_path.format(\n",
    "    model=model_name,\n",
    "    examples=N_EXAMPLES,\n",
    "    bulk=N_REQS,\n",
    "    time=time.strftime('%m.%d.%Y-%H:%M:%S'),\n",
    "    accuracy=round(accuracy, 3)\n",
    ")\n",
    "\n",
    "results_file = base_path / results_path\n",
    "results_file.parent.mkdir(exist_ok=True)\n",
    "results_file.touch()\n",
    "\n",
    "with results_file.open(\"w\") as f:\n",
    "    json.dump({\"accuracy\": accuracy,\n",
    "        \"number_of_reqs\": number_of_reqs,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"total_completion_tokens\": total_completion_tokens,\n",
    "        \"avg_token_per_req\": avg_token_per_req,\n",
    "        \"avg_completion_token_per_req\": avg_completion_token_per_req,\n",
    "        \"avg_time_per_req\": avg_time_per_req,\n",
    "        \"examples\": examples,\n",
    "        \"responses\": results}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
